{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Handwritten Digit Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the images and reshape the data arrays to have a single color channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "(trainX,trainy),(testX,testy) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(60000, 28, 28), y=(60000,)\n"
     ]
    }
   ],
   "source": [
    "print('Train: X=%s, y=%s' % (trainX.shape,trainy.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape dataset to have single channel\n",
    "\n",
    "trainX=trainX.reshape(trainX.shape[0],28,28,1)\n",
    "testX=testX.reshape(testX.shape[0],28,28,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encode target values\n",
    "from keras.utils import to_categorical\n",
    "trainy = to_categorical(trainy)\n",
    "testy = to_categorical(testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load_dataset() function implements these behaviors and can be used to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    #Load dataset\n",
    "    (trainX,trainY),(testX,testY) = mnist.load_dataset()\n",
    "    \n",
    "    #Reshape\n",
    "    trainX=trainX.reshape(trainX.shape[0],28,28,1)\n",
    "    trainY=trainY.reshape(trainY.shape[0],28,28,1)\n",
    "    \n",
    "    #One hot encode\n",
    "    testX = to_categorical(testX)\n",
    "    testY = to_categorical(testY)\n",
    "    \n",
    "    return trainX,trainY,testX,testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Pixel Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good starting point is to normalize the pixel values of grayscale images, \n",
    "e.g. rescale them to the range [0,1]. This involves first converting the data type from unsigned \n",
    "integers to floats,then dividing the pixel values by the maximum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert from integer to float\n",
    "train_norm = trainX.astype('float32')\n",
    "test_norm = testX.astype('float32')\n",
    "\n",
    "#normalize to range 0 to 1\n",
    "train_norm = train_norm / 255.0\n",
    "test_norm = test_norm / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put everything in prep_pixels() function\n",
    "#Scale_pixels\n",
    "def prep_pixels(train,test):\n",
    "    #Convert from integer to float\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "\n",
    "    #normalize to range 0 to 1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    \n",
    "    return train_norm,test_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has two main aspects: the feature extraction front end comprised of convolutional and pooling layers, and the classifier backend that will make a prediction.\n",
    "\n",
    "For the convolutional front-end, we can start with a single convolutional layer with a small filter size (3,3) and a modest number of filters (32) followed by a max pooling layer. The filter maps can then be flattened to provide features to the classifier.\n",
    "\n",
    "Given that the problem is a multi-class classification task, we know that we will require an output layer with 10 nodes in order to predict the probability distribution of an image belonging to each of the 10 classes. This will also require the use of a softmax activation function. Between the feature extractor and the output layer, we can add a dense layer to interpret the features, in this case with 100 nodes.\n",
    "\n",
    "All layers will use the ReLU activation function and the He weight initialization scheme, both best practices.\n",
    "\n",
    "We will use a conservative configuration for the stochastic gradient descent optimizer with a learning rate of 0.01 and a momentum of 0.9. The categorical cross-entropy loss function will be optimized, suitable for multi-class classification, and we will monitor the classification accuracy metric, which is appropriate given we have the same number of examples in each of the 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,Dense,MaxPooling2D,Flatten\n",
    "from keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',input_shape=(28,28,1)))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100,activation='relu',kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "    \n",
    "    #Compile model\n",
    "    opt = SGD(learning_rate=0.01,momentum=0.9)\n",
    "    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics='accuracy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be evaluated using five-fold cross-validation. The value of k=5 was chosen to provide a baseline for both repeated evaluation and to not be so large as to require a long running time. Each test set will be 20% of the training dataset, or about 12,000 examples, close to the size of the actual test set for this problem.\n",
    "\n",
    "The training dataset is shuffled prior to being split, and the sample shuffling is performed each time, so that any model we evaluate will have the same train and test datasets in each fold, providing an apples-to-apples comparison between models.\n",
    "\n",
    "We will train the baseline model for a modest 10 training epochs with a default batch size of 32 examples. The test set for each fold will be used to evaluate the model both during each epoch of the training run, so that we can later create learning curves, and at the end of the run, so that we can estimate the performance of the model. As such, we will keep track of the resulting history from each run, as well as the classification accuracy of the fold.\n",
    "\n",
    "The evaluate_model() function below implements these behaviors, taking the training dataset as arguments and returning a list of accuracy scores and training histories that can be later summarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate a model using K-fold cross validation\n",
    "def model_evaluate(dataX,dataY,n_fold=5):\n",
    "    scores,histories = list(),list()\n",
    "    \n",
    "    #Prepare cross validation\n",
    "    kfold = KFold(n_splits=n_fold,shuffle=True,random_state=1)\n",
    "    \n",
    "    #enumarate splits\n",
    "    for train_ix,test_ix in kfold.split(dataX):\n",
    "        #define model\n",
    "        model = define_model()\n",
    "        \n",
    "        #select rows for train and test\n",
    "        trainX,trainY,testX,testY = dataX[train_ix],dataY[train_ix],dataX[test_ix],dataY[test_ix]\n",
    "        \n",
    "        #fit model\n",
    "        history = model.fit(trainX,trainY,epochs=10,batch_size=32,validation_data=(testX,testY),verbose=0)\n",
    "        \n",
    "        #evaluate model\n",
    "        _,acc = model.evaluate(testX,testY,verbose=0)\n",
    "        print('> %.3f',(acc*100.0))\n",
    "        \n",
    "        #store scores\n",
    "        scores.append(acc)\n",
    "        histories.append(history)\n",
    "        \n",
    "        return scores,histories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
